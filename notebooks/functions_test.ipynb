{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#from matplotlib import scatter_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function to Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_CollegeScorecard(columns):\n",
    "    \"\"\"\n",
    "    Read in columns from files from inside the zip file. \n",
    "    Also assign a year to each DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    columns: list,\n",
    "        columns is a list of strings matching the desired column headers\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sheets: dictionary,\n",
    "        sheets is a dictionary of year:DataFrame pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    zip_file = ZipFile('CollegeScorecard_Raw_Data.zip')\n",
    "    \n",
    "    sheets = {}\n",
    "    for year in range(1996, 2018):\n",
    "        acyear = str(year)+'_'+str(year+1)[-2:]\n",
    "        sheets[year] = pd.read_csv(zip_file.open('CollegeScorecard_Raw_Data/MERGED'+acyear+'_PP.csv'), usecols=columns)\n",
    "        sheets[year]['YEAR'] = year\n",
    "        sheets[year]['YEAR'] = pd.to_datetime(sheets[year]['YEAR'], format='%Y')\n",
    "    return sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['INSTNM', 'HIGHDEG', 'CONTROL', 'REGION', 'LOCALE', 'LO_INC_DEBT_N', 'MD_INC_DEBT_N', 'HI_INC_DEBT_N',\n",
    "           'LOAN_EVER', 'PELL_EVER', 'PCTPELL', 'ICLEVEL', 'CURROPER', 'TUITFTE', 'CDR3', 'INEXPFTE']\n",
    "sheets = read_in_CollegeScorecard(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create concatenated df of all sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_all_sheets(sheets):\n",
    "    \"\"\"\n",
    "    Concatenates DataFrames in a dictionary of DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sheets: dictionary,\n",
    "        key value pairs are year and DataFrame associated to that year\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    full_df: DataFrame\n",
    "    \"\"\"\n",
    "    for year, df in sheets.items():\n",
    "        df['iyear'] = df['YEAR']\n",
    "        if year==1996:\n",
    "            full_df = df.set_index([df.index, 'iyear'])\n",
    "        else:\n",
    "            full_df = pd.concat([full_df, df.set_index([df.index, 'iyear'])])\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = concatenate_all_sheets(sheets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Fill huge DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_fill_from_year(df, year):\n",
    "    fill_values = {}\n",
    "    for name in df.INSTNM.unique():\n",
    "        fill_values[name] = {'LOCALE': df.loc[(df.INSTNM==name)].LOCALE.values[0], \n",
    "                             'CURROPER': df.loc[(df.INSTNM==name)].CURROPER.values[0],\n",
    "                             'CONTROL': df.loc[(df.INSTNM==name)].CONTROL.values[0]}\n",
    "    \n",
    "    for name in df.loc[df.YEAR!=year].INSTNM.unique():\n",
    "        fill_values[name] = fill_values.get(name, {'LOCALE': np.NaN, 'CURROPER': np.NaN, 'CONTROL': np.NaN})\n",
    "        \n",
    "    copy_df = df.drop(['LOCALE', 'CURROPER', 'CONTROL'], axis=1)\n",
    "    \n",
    "    copy_df['LOCALE'] = df.INSTNM.map(lambda name: fill_values[name]['LOCALE'])\n",
    "    copy_df['CURROPER'] = df.INSTNM.map(lambda name: fill_values[name]['CURROPER'])\n",
    "    copy_df['CONTROL'] = df.INSTNM.map(lambda name: fill_values[name]['CONTROL'])\n",
    "    \n",
    "    return copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016_2017 = back_fill_from_year(full_df.loc[(full_df.YEAR==2016)|(full_df.YEAR==2017)], 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_clean(df, columns=None):\n",
    "    \"\"\"\n",
    "    drops rows with NaNs in the columns given. \n",
    "    If no columns are given, drops all rows with any nans.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: DataFrame\n",
    "    \n",
    "    columns: list\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df_nonan: dataframe\n",
    "    \"\"\"\n",
    "    df_unsuppressed = df.replace('PrivacySuppressed', np.NaN)\n",
    "    df_nonan = df.dropna(subset=columns)\n",
    "    return df_nonan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_add_feature(df):\n",
    "    df['Public_or_Private'] = df.CONTROL.map({2:0, 1:1, 3:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sheets[2017].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logistic_regression(df, predictor_columns, predicted_column):\n",
    "    \"\"\"\n",
    "    Creates a logistic regression from input column names to predictor column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "        the dataframe's columns should include the predictor_columns and predicted_column\n",
    "    \n",
    "    predictor_columns: list\n",
    "        Should be a subset of columns from df. \n",
    "        Should have empty intersection with predicted_column\n",
    "    \n",
    "    predicted_column: string\n",
    "        Should be an element in the list of columns from df. \n",
    "        Should not be included in predictor_columns\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    logreg: logistic regression already trained on training data from predictor columns\n",
    "    \n",
    "    Note: This function will change the given dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create dataframes( or series) with predictors and  predicted values\n",
    "    X = df[predictor_columns]\n",
    "    y = df[predicted_column]\n",
    "    \n",
    "    # Scale the data using Robust Scaler\n",
    "    scale = RobustScaler()\n",
    "    transformed = scale.fit_transform(X)\n",
    "    X = pd.DataFrame(transformed, columns = X.columns)\n",
    "    \n",
    "    # Create Train and Test Data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "    \n",
    "    # Create a logistic regression model\n",
    "    logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='lbfgs')\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    try:\n",
    "        model_log = logreg.fit(X_train, y_train)\n",
    "    except:\n",
    "        model_log = logreg.fit(np.array(X_train).reshape(-1,1), y_train)\n",
    "        \n",
    "    # Add new columns to the given data frame with predicted values and probability of correct predictions\n",
    "    try:\n",
    "        df['Predicted_'+predicted_column] = logreg.predict(X)\n",
    "        df['ProbCorrect_Predicted_'+predicted_column] = logreg.predict_proba(X)[:,1]\n",
    "    except:\n",
    "        df['Predicted_'+predicted_column] = logreg.predict(np.array(X).reshape(-1,1))\n",
    "        df['ProbCorrect_Predicted_'+predicted_column] = logreg.predict_proba(np.array(X).reshape(-1,1))[:,1]\n",
    "    \n",
    "    return logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test create_logistic_regression\n",
    "\n",
    "create_logistic_regression(df, ['CDR3', 'HI_INC_DEBT_N'], 'Public_or_Private'\n",
    "                          ).score(df[['CDR3', 'HI_INC_DEBT_N']], df['Public_or_Private'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
